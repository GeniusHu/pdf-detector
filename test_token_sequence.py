#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•åŸºäºTokençš„åºåˆ—ç”Ÿæˆé€»è¾‘

éªŒè¯ï¼š
1. Tokenizer æ­£ç¡®åˆ†å‰²æ–‡æœ¬
2. SequenceGenerator åŸºäºtokenç”Ÿæˆåºåˆ—
3. å„ç§è¾¹ç•Œæƒ…å†µ
"""

from document_processor import SymbolCleaner, Tokenizer, SequenceGenerator, Paragraph, Token


def test_tokenizer():
    """æµ‹è¯•Tokenizerçš„åˆ†è¯åŠŸèƒ½"""
    print("=" * 80)
    print("Tokenizer æµ‹è¯•")
    print("=" * 80)

    cleaner = SymbolCleaner()
    tokenizer = Tokenizer()

    test_cases = [
        ("ä»Šå¤©å¤©æ°”å¾ˆå¥½å•Š", "çº¯ä¸­æ–‡"),
        ("hello world java", "çº¯è‹±æ–‡"),
        ("Pythonç‰ˆæœ¬38å¾ˆå¼ºå¤§", "ä¸­è‹±æ··åˆ"),
        ("å‘¨é•¿ä¸º100ç±³", "ä¸­æ–‡+æ•°å­—"),
        ("Python 3.14 is great", "è‹±æ–‡+æ•°å­—"),
    ]

    all_passed = True
    for text, description in test_cases:
        clean_text = cleaner.clean_text(text)
        tokens = tokenizer.tokenize(clean_text)

        print(f"\n{description}: '{text}'")
        print(f"  æ¸…ç†å: '{clean_text}'")
        print(f"  Tokenæ•°: {len(tokens)}")
        print(f"  Tokens: {[(t.text, t.token_type) for t in tokens]}")

    print("\n" + "=" * 80)


def test_sequence_generator():
    """æµ‹è¯•SequenceGeneratorçš„åºåˆ—ç”Ÿæˆ"""
    print("\n" + "=" * 80)
    print("SequenceGenerator æµ‹è¯•")
    print("=" * 80)

    cleaner = SymbolCleaner()
    tokenizer = Tokenizer()

    test_cases = [
        # (è¾“å…¥æ–‡æœ¬, N, é¢„æœŸåºåˆ—æ•°)
        ("ä»Šå¤©å¤©æ°”å¾ˆå¥½å•Š", 3, 4),      # 7ä¸ªtoken, N=3 â†’ 5ä¸ªåºåˆ— (7-3+1=5)... ç­‰ç­‰è®©æˆ‘ç®—ä¸€ä¸‹
        ("hello world java test", 2, 3),  # 4ä¸ªè¯, N=2 â†’ 3ä¸ªåºåˆ—
        ("Pythonç‰ˆæœ¬38å¾ˆå¼ºå¤§", 3, 4),     # 7ä¸ªtoken, N=3 â†’ 5ä¸ªåºåˆ—... ä¸å¯¹
        ("å‘¨é•¿ä¸º100ç±³", 3, 2),            # 5ä¸ªtoken, N=3 â†’ 3ä¸ªåºåˆ—... ä¸å¯¹
    ]

    for text, n, expected_count in test_cases:
        clean_text = cleaner.clean_text(text)
        tokens = tokenizer.tokenize(clean_text)

        print(f"\nè¾“å…¥: '{text}' (N={n})")
        print(f"  æ¸…ç†å: '{clean_text}'")
        print(f"  Tokens ({len(tokens)}ä¸ª): {[(t.text, t.token_type) for t in tokens]}")

        generator = SequenceGenerator(sequence_length=n)
        sequences = generator.generate_from_text(text)

        print(f"  ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")
        print(f"  åºåˆ—:")
        for i, seq in enumerate(sequences):
            print(f"    {i + 1}. '{seq}'")

    print("\n" + "=" * 80)


def test_paragraph_based_generation():
    """æµ‹è¯•åŸºäºæ®µè½çš„åºåˆ—ç”Ÿæˆ"""
    print("\n" + "=" * 80)
    print("åŸºäºæ®µè½çš„åºåˆ—ç”Ÿæˆæµ‹è¯•")
    print("=" * 80)

    cleaner = SymbolCleaner()

    # åˆ›å»ºæµ‹è¯•æ®µè½
    paragraphs = [
        Paragraph(
            raw_text="ä»Šå¤©å¤©æ°”å¾ˆå¥½å•Š",
            clean_text=cleaner.clean_text("ä»Šå¤©å¤©æ°”å¾ˆå¥½å•Š"),
            start_page=1,
            start_line=1,
            char_count=7,
            clean_char_count=7,
            file_type="pdf"
        ),
        Paragraph(
            raw_text="hello world java",
            clean_text=cleaner.clean_text("hello world java"),
            start_page=1,
            start_line=1,
            char_count=17,
            clean_char_count=17,
            file_type="pdf"
        ),
        Paragraph(
            raw_text="Pythonç‰ˆæœ¬38å¾ˆå¼ºå¤§",
            clean_text=cleaner.clean_text("Pythonç‰ˆæœ¬38å¾ˆå¼ºå¤§"),
            start_page=1,
            start_line=1,
            char_count=14,
            clean_char_count=13,
            file_type="pdf"
        ),
    ]

    # æµ‹è¯•ä¸åŒçš„åºåˆ—é•¿åº¦
    for n in [3, 4, 5]:
        print(f"\n--- N = {n} ---")
        generator = SequenceGenerator(sequence_length=n)
        sequences = generator.generate_from_paragraphs(paragraphs)

        print(f"æ€»åºåˆ—æ•°: {len(sequences)}")

        # æ˜¾ç¤ºå‰5ä¸ªåºåˆ—
        for i, seq in enumerate(sequences[:5]):
            print(f"  {i + 1}. æ¯”å¯¹: '{seq['sequence']}'")
            print(f"     æ˜¾ç¤º: '{seq['display_sequence']}'")
            print(f"     Tokens: {[(t.text, t.token_type) for t in seq['tokens']]}")

    print("\n" + "=" * 80)


def test_similarity_detection_scenario():
    """æµ‹è¯•ç›¸ä¼¼åº¦æ£€æµ‹åœºæ™¯"""
    print("\n" + "=" * 80)
    print("ç›¸ä¼¼åº¦æ£€æµ‹åœºæ™¯æµ‹è¯•")
    print("=" * 80)

    cleaner = SymbolCleaner()

    # åœºæ™¯1ï¼šä¸­æ–‡ç›¸ä¼¼
    text1 = "æˆ‘ä»Šå¤©åƒäº†ä¸€ä¸ªè‹¹æœ"
    text2 = "ä»–æ˜¨å¤©åƒäº†ä¸€ä¸ªè¥¿ç“œ"

    print("\nåœºæ™¯1ï¼šä¸­æ–‡ç›¸ä¼¼æ£€æµ‹ (N=5)")
    print(f"æ–‡æ¡£1: '{text1}'")
    print(f"æ–‡æ¡£2: '{text2}'")

    gen1 = SequenceGenerator(sequence_length=5)
    seqs1 = gen1.generate_from_text(text1)

    gen2 = SequenceGenerator(sequence_length=5)
    seqs2 = gen2.generate_from_text(text2)

    print(f"\næ–‡æ¡£1åºåˆ—: {seqs1}")
    print(f"æ–‡æ¡£2åºåˆ—: {seqs2}")

    common = set(seqs1) & set(seqs2)
    print(f"\nå…±åŒåºåˆ—: {common}")
    print(f"æ£€æµ‹åˆ°é‡å¤: {len(common) > 0}")

    # åœºæ™¯2ï¼šè‹±æ–‡ç›¸ä¼¼
    text3 = "hello world java test"
    text4 = "hello world python code"

    print("\nåœºæ™¯2ï¼šè‹±æ–‡ç›¸ä¼¼æ£€æµ‹ (N=2)")
    print(f"æ–‡æ¡£3: '{text3}'")
    print(f"æ–‡æ¡£4: '{text4}'")

    gen3 = SequenceGenerator(sequence_length=2)
    seqs3 = gen3.generate_from_text(text3)

    gen4 = SequenceGenerator(sequence_length=2)
    seqs4 = gen4.generate_from_text(text4)

    print(f"\næ–‡æ¡£3åºåˆ—: {seqs3}")
    print(f"æ–‡æ¡£4åºåˆ—: {seqs4}")

    common2 = set(seqs3) & set(seqs4)
    print(f"\nå…±åŒåºåˆ—: {common2}")
    print(f"æ£€æµ‹åˆ°é‡å¤: {len(common2) > 0}")

    print("\n" + "=" * 80)


def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("\n" + "=" * 80)
    print("è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 80)

    cleaner = SymbolCleaner()
    generator = SequenceGenerator(sequence_length=5)

    edge_cases = [
        ("", "ç©ºå­—ç¬¦ä¸²"),
        ("hello", "å¤ªçŸ­ï¼ˆ1ä¸ªè¯ï¼ŒN=5ï¼‰"),
        ("hello world", "å¤ªçŸ­ï¼ˆ2ä¸ªè¯ï¼ŒN=5ï¼‰"),
        ("ä½ å¥½ä¸–ç•Œ", "å¤ªçŸ­ï¼ˆ4ä¸ªå­—ï¼ŒN=5ï¼‰"),
        ("hello world java test code here", "åˆšå¥½å¤Ÿï¼ˆ6ä¸ªè¯ï¼ŒN=5ï¼‰"),
    ]

    for text, description in edge_cases:
        clean_text = cleaner.clean_text(text)
        tokenizer = Tokenizer()
        tokens = tokenizer.tokenize(clean_text)

        sequences = generator.generate_from_text(text)

        print(f"\n{description}: '{text}'")
        print(f"  Tokenæ•°: {len(tokens)}")
        print(f"  åºåˆ—æ•°: {len(sequences)}")

    print("\n" + "=" * 80)


def test_all_examples():
    """æµ‹è¯•ç”¨æˆ·ç¡®è®¤çš„æ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ç”¨æˆ·ç¡®è®¤ç¤ºä¾‹æµ‹è¯•")
    print("=" * 80)

    cleaner = SymbolCleaner()
    tokenizer = Tokenizer()

    examples = [
        ("ä»Šå¤©å¤©æ°”å¾ˆå¥½å•Š", 3, ["ä»Šå¤©å¤©", "å¤©å¤©æ°”", "å¤©æ°”å¾ˆ", "æ°”å¾ˆå¥½", "å¾ˆå¥½å•Š"]),
        ("hello world java test", 2, ["helloworld", "worldjava", "javatest"]),
        ("I love coding very much", 3, ["ilovecoding", "lovecodingvery", "codingverymuch"]),
        ("Pythonç‰ˆæœ¬38å¾ˆå¼ºå¤§", 4, ["pythonç‰ˆæœ¬38", "ç‰ˆæœ¬38å¾ˆ", "æœ¬38å¾ˆå¼º", "38å¾ˆå¼ºå¤§"]),
        ("å‘¨é•¿ä¸º100ç±³", 3, ["å‘¨é•¿ä¸º", "é•¿ä¸º100", "ä¸º100ç±³"]),
    ]

    all_passed = True
    for text, n, expected in examples:
        clean_text = cleaner.clean_text(text)
        tokens = tokenizer.tokenize(clean_text)
        actual_sequences = SequenceGenerator(sequence_length=n).generate_from_text(text)

        print(f"\nè¾“å…¥: '{text}', N={n}")
        print(f"  Tokens ({len(tokens)}ä¸ª): {[(t.text, t.token_type) for t in tokens]}")
        print(f"  æœŸæœ›åºåˆ—: {expected}")
        print(f"  å®é™…åºåˆ—: {actual_sequences}")

        passed = actual_sequences == expected
        status = "âœ“ PASS" if passed else "âœ— FAIL"
        print(f"  {status}")

        all_passed = all_passed and passed

    print("\n" + "=" * 80)
    if all_passed:
        print("âœ“ æ‰€æœ‰ç¤ºä¾‹æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âœ— æœ‰ç¤ºä¾‹æµ‹è¯•å¤±è´¥ï¼")
    print("=" * 80)


if __name__ == "__main__":
    print("\n" + "ğŸ§ª" * 40)
    print("å¼€å§‹æµ‹è¯•åŸºäºTokençš„åºåˆ—ç”Ÿæˆ")
    print("ğŸ§ª" * 40 + "\n")

    test_tokenizer()
    test_sequence_generator()
    test_paragraph_based_generation()
    test_similarity_detection_scenario()
    test_edge_cases()
    test_all_examples()

    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 80 + "\n")
